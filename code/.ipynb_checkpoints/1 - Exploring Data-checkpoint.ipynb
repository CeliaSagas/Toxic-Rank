{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import contractions\n",
    "from datetime import datetime\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from itertools import combinations, combinations_with_replacement, permutations\n",
    "from textblob.translate import NotTranslated\n",
    "from pandas_profiling import ProfileReport\n",
    "import time\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv(\"./jigsaw-toxic-severity-rating/validation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b69f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of unique workers \n",
    "data['worker'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of all comments in the dataset \n",
    "\n",
    "less_toxic = data['less_toxic']\n",
    "more_toxic = data['more_toxic']\n",
    "comments = pd.concat([less_toxic, more_toxic])\n",
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of all unique comments in the dataset\n",
    "\n",
    "unique_comments = [word for word in comments.unique()]\n",
    "print(len(unique_comments))\n",
    "print(type(unique_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834139f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of terms: \" + str(sum([len(d.split(' ')) for d in unique_comments])))\n",
    "print(\"Number of documents: \" + str(len(unique_comments)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~__'\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "from google_trans_new import google_translator  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_token(text):\n",
    "    \n",
    "    \n",
    "    text = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "    text = [word_tokenize(word) for word in text]\n",
    "    text = [item for sublist in text for item in sublist]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = [re.sub(r'http\\S+', '', each) for each in text]\n",
    "    text = [re.sub('[0-9+]', '', each) for each in text]\n",
    "    text = [re.sub('_', '', each) for each in text]\n",
    "    text = [re.sub(\"\\n\",\"\",each) for each in text]\n",
    "    text = [re.sub('/_/g', '', each) for each in text]\n",
    "    text = [re.sub('[^\\u0000-\\u05C0\\u2100-\\u214F]+', '', each) for each in text]\n",
    "    text = [re.sub('[\\u0401\\u0451\\u0410-\\u044f]', '', each) for each in text]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [\"\".join(dict.fromkeys(word)) for word in text]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [to_token(comment) for comment in unique_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tfidf matrix \n",
    "cv_tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "trans = cv_tfidf.fit_transform(tokens).toarray()\n",
    "tfidf_formed = pd.DataFrame(trans, columns = cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caadf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CV matrix \n",
    "cv = CountVectorizer()\n",
    "corpus = [str (item) for item in tokens]\n",
    "trans = cv.fit_transform(corpus)\n",
    "cv_formed = pd.DataFrame(trans.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create plot topic function \n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    fig.savefig(str(title[-16:-1])+\".png\",  facecolor='white', edgecolor='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "count_vec = NMF(10)\n",
    "nmf_5=count_vec.fit(cv_formed)\n",
    "tfidf_feature_names = cv.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    count_vec, tfidf_feature_names, 10, \"Topics in NMF model with Count Vectorizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nmf = NMF(10)\n",
    "nmf_5=nmf.fit(tfidf_formed)\n",
    "tfidf_feature_names = cv_tfidf.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf, tfidf_feature_names, 10, \"Topics in NMF model with TFIDF Vectorizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ab508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
