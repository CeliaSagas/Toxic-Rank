{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import contractions\n",
    "from datetime import datetime\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "from itertools import combinations, combinations_with_replacement, permutations\n",
    "from textblob.translate import NotTranslated\n",
    "from pandas_profiling import ProfileReport\n",
    "import time\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tqdm\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from google_trans_new import google_translator  \n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection, metrics   \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv(\"./jigsaw-toxic-severity-rating/validation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b69f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view data\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create distinct df's for more toxic and less toxic comments\n",
    "\n",
    "less_toxic = data.drop('more_toxic', axis=1)\n",
    "more_toxic = data.drop('less_toxic', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add target column to both \n",
    "\n",
    "less_toxic['target'] = 0\n",
    "more_toxic['target'] = 1\n",
    "\n",
    "#rename text column for both \n",
    "\n",
    "less_toxic = less_toxic.rename(columns={'less_toxic':'text'})\n",
    "more_toxic = more_toxic.rename(columns={'more_toxic':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd56a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejoin data \n",
    "\n",
    "comments = pd.concat([less_toxic, more_toxic], axis =0)\n",
    "\n",
    "#randomly reorder\n",
    "\n",
    "comments = comments.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#create average target score for comments \n",
    "comments = comments.groupby('text')['target'].mean().reset_index()\n",
    "\n",
    "#shape\n",
    "print(comments.head())\n",
    "\n",
    "#save comments\n",
    "\n",
    "comments.to_csv(\"comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of toxic values per comment \n",
    "%matplotlib inline\n",
    "sns.histplot(data=comments, x=\"target\", binwidth=.33)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44985a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remap target values \n",
    "\n",
    "comments['target'] = (comments['target'] > 0) * 1\n",
    "\n",
    "#histogram of toxic values per comment \n",
    "%matplotlib inline\n",
    "sns.histplot(data=comments, x=\"target\", binwidth=.5)\n",
    "plt.show()\n",
    "\n",
    "# print(comments['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text clean + tokenizer function\n",
    "\n",
    "punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~__'\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "cv_tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_token(text):\n",
    "    \n",
    "    \n",
    "    text = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "    text = [word_tokenize(word) for word in text]\n",
    "    text = [item for sublist in text for item in sublist]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = [re.sub(r'http\\S+', '', each) for each in text]\n",
    "    text = [re.sub('[0-9+]', '', each) for each in text]\n",
    "    text = [re.sub('_', '', each) for each in text]\n",
    "    text = [re.sub(\"\\n\",\"\",each) for each in text]\n",
    "    text = [re.sub('/_/g', '', each) for each in text]\n",
    "    text = [re.sub('[^\\u0000-\\u05C0\\u2100-\\u214F]+', '', each) for each in text]\n",
    "    text = [re.sub('[\\u0401\\u0451\\u0410-\\u044f]', '', each) for each in text]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [\"\".join(dict.fromkeys(word)) for word in text]\n",
    "\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "\n",
    "X, y = comments['text'], comments['target']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c358c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of all unique comments in the dataset\n",
    "\n",
    "corpus = [word for word in X]\n",
    "\n",
    "\n",
    "# #tokenize train and test sets \n",
    "\n",
    "features = [to_token(comment) for comment in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition data into test data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = .2, random_state = 33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dbc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(X_train,y_train,model):\n",
    "    \n",
    "    #length of training set   \n",
    "    data_length= len(X_train)\n",
    "    \n",
    "    # prepare the cross-validation procedure\n",
    "    cv = KFold(n_splits=5, random_state=33, shuffle=True)\n",
    "    \n",
    "    #create the model\n",
    "    if model == 'Extra Trees':\n",
    "        clf_model = ExtraTreesClassifier(n_estimators=1000, random_state=33)\n",
    "    elif model == 'Random Forest':\n",
    "        clf_model = RandomForestClassifier(n_estimators=1000, random_state=33)\n",
    "    elif model == 'Logistic Regression':\n",
    "        clf_model = LogisticRegression(max_iter=10000, solver = 'lbfgs',random_state=33)\n",
    "       \n",
    "    \n",
    "    elif model == 'Bayes':\n",
    "        clf_model = GaussianNB()\n",
    "    elif model == 'XGBoost':\n",
    "        clf_model = XGBClassifier(use_label_encoder=False, verbosity=0)\n",
    "        \n",
    "  \n",
    "    \n",
    "    #make pipeline \n",
    "    clf = Pipeline(steps=[('tfidf', TfidfVectorizer(preprocessor=' '.join)),\n",
    "                          ('svd', TruncatedSVD(n_components=25, random_state=33)), \n",
    "                          ('scaler', StandardScaler()),\n",
    "                          ('classifier', clf_model)])\n",
    "    \n",
    "    scoring = ['roc_auc','accuracy','precision', 'recall', 'f1']\n",
    "  \n",
    "    # evaluate model\n",
    "    scores = cross_validate(clf, X_train, y_train, \n",
    "                            scoring=scoring, \n",
    "                            cv=cv, n_jobs=-1)\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"The ROC AUC for each fold are: \"+str(scores['test_roc_auc']),\n",
    "        \"The Accuracy Scores for each fold are: \"+str(scores['test_accuracy']),\n",
    "          \"\\nThe Precision Sores for each fold are: \"+str(scores['test_precision']),\n",
    "          \"\\nThe Recall Sores for each fold are: \"+str(scores['test_recall']),\n",
    "          \"\\nThe F1 Sores for each fold are: \"+str(scores['test_f1'])\n",
    "          \n",
    "         )\n",
    "    #create df of results \n",
    "    df = pd.DataFrame(scores)\n",
    "    df['model'] = model\n",
    "    \n",
    "    #reorder columns\n",
    "    cols = list(df.columns)\n",
    "    cols = [cols[-1]] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb073c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train extra trees\n",
    "\n",
    "train_1 = run_training(X_train,y_train,'Extra Trees')\n",
    "train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train random forest \n",
    "train_2 = run_training(X_train,y_train, 'Random Forest')\n",
    "train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cae6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression\n",
    "train_3 = run_training(X_train, y_train, 'Logistic Regression')\n",
    "train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7686d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train bayes\n",
    "train_4= run_training(X_train, y_train, 'Bayes')\n",
    "train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d232145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train xgboost\n",
    "train_5 = run_training(X_train, y_train ,'XGBoost')\n",
    "train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create results df\n",
    "frames = [train_1, train_2,train_3,train_4,train_5]\n",
    "train_results = pd.concat(frames)\n",
    "\n",
    "#create average results df \n",
    "train_average=train_results.groupby('model')[['test_roc_auc','test_accuracy','test_precision','test_recall', 'test_f1']].mean().reset_index()\n",
    "\n",
    "train_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19000e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the F1 Score Average\n",
    "\n",
    "#color scheme\n",
    "pal = sns.color_palette(\"winter_r\", (len(train_average)))\n",
    "rank = train_average[\"test_f1\"].argsort().argsort() \n",
    "\n",
    "#sort the df by f1\n",
    "train_average = train_average.sort_values('test_f1')\n",
    "\n",
    "#plot the data\n",
    "sns.barplot(x=train_average.model, y= train_average.test_f1, palette=np.array(pal[::-1])[rank])\n",
    "\n",
    "#titles\n",
    "plt.xlabel(\"Test\")\n",
    "plt.ylabel(\"F-Score\")\n",
    "plt.title(\"Average F Score for Cross Validation on Training Data\")\n",
    "\n",
    "\n",
    "#save the plot \n",
    "plt.savefig('F_score_cv.png', dpi=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb619b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the Accuracy Average\n",
    "\n",
    "#color scheme\n",
    "pal = sns.color_palette(\"magma_r\", (len(train_average)))\n",
    "rank = train_average[\"test_accuracy\"].argsort().argsort() \n",
    "\n",
    "#sort the df by f1\n",
    "train_average = train_average.sort_values('test_accuracy')\n",
    "\n",
    "#plot the data\n",
    "sns.barplot(x=train_average.model, y= train_average.test_accuracy, palette=np.array(pal[::-1])[rank])\n",
    "\n",
    "#titles\n",
    "plt.xlabel(\"Test\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Average ROC AUC for Cross Validation on Sample Data\")\n",
    "\n",
    "\n",
    "#save the plot \n",
    "plt.savefig('Accuracy_score_cv.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ROC AUC\n",
    "\n",
    "#color scheme\n",
    "pal = sns.color_palette(\"magma_r\", (len(train_average)))\n",
    "rank = train_average[\"test_roc_auc\"].argsort().argsort() \n",
    "\n",
    "#sort the df by f1\n",
    "train_average = train_average.sort_values('test_roc_auc')\n",
    "\n",
    "#plot the data\n",
    "sns.barplot(x=train_average.model, y= train_average.test_roc_auc, palette=np.array(pal[::-1])[rank])\n",
    "\n",
    "#titles\n",
    "plt.xlabel(\"Test\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.title(\"Average ROC AUC for Cross Validation on Sample Data\")\n",
    "\n",
    "\n",
    "#save the plot \n",
    "plt.savefig('rocauc_score_cv.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ed756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
