{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9942552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"lr_output.csv\")\n",
    "mapper = {\"Unnamed: 0\": \"Comment_ID\"}\n",
    "data=data.rename(columns=mapper)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load comments \n",
    "comments = pd.read_csv(\"comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d657b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate X and y in comments df\n",
    "X, y = comments['text'], comments['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text clean + tokenizer function\n",
    "\n",
    "punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~__'\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop = stopwords.words('english')\n",
    "stop.append('the')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "cv_tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_token(text):\n",
    "    \n",
    "   \n",
    "    text = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "    text = [word_tokenize(word) for word in text]\n",
    "    text = [item for sublist in text for item in sublist]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = [re.sub(r'http\\S+', '', each) for each in text]\n",
    "    text = [re.sub('[0-9+]', '', each) for each in text]\n",
    "    text = [re.sub('_', '', each) for each in text]\n",
    "    text = [re.sub(\"\\n\",\"\",each) for each in text]\n",
    "    text = [re.sub('/_/g', '', each) for each in text]\n",
    "    text = [re.sub('[^\\u0000-\\u05C0\\u2100-\\u214F]+', '', each) for each in text]\n",
    "    text = [re.sub('[\\u0401\\u0451\\u0410-\\u044f]', '', each) for each in text]\n",
    "    text = [w for w in text if not w.lower() in stop]\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [\"\".join(dict.fromkeys(word)) for word in text]\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd43239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of all unique comments in the dataset\n",
    "\n",
    "corpus = [word for word in X]\n",
    "\n",
    "\n",
    "# #tokenize train and test sets \n",
    "\n",
    "features = [to_token(comment) for comment in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tfidf vector matrix for all comments\n",
    "cv = CountVectorizer()\n",
    "feat = [str (item) for item in features]\n",
    "trans = cv.fit_transform(feat)\n",
    "cv_df = pd.DataFrame(trans.toarray(), columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create wordcloud of comments classified as toxic \n",
    "toxic = data[data['model_probability']>.95]\n",
    "toxic =toxic['Comment_ID']\n",
    "\n",
    "toxic_comments = cv_df.iloc[toxic,:].sum()\n",
    "import random\n",
    "def red_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 100%%, %d%%)\" % random.randint(30, 60)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(toxic_comments)\n",
    "plt.imshow(wordcloud.recolor(color_func=red_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.savefig(\"toxic_wordcloud.png\", dpi=400)\n",
    "#plt.imshow(wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228feb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create wordcloud of comments classified as not toxic \n",
    "not_toxic = data[data['model_probability']<0.4]\n",
    "not_toxic =not_toxic['Comment_ID']\n",
    "\n",
    "def blue_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(236, 100%%, %d%%)\" % random.randint(30, 100)\n",
    "\n",
    "not_toxic_comments = cv_df.iloc[not_toxic,:].sum()\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(not_toxic_comments)\n",
    "plt.imshow(wordcloud.recolor(color_func=blue_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "\n",
    "plt.savefig(\"not_toxic_wordcloud.png\", dpi=400)\n",
    "#plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_top20=pd.DataFrame(toxic_comments.T.sort_values(ascending=False)[0:50])\n",
    "fig = px.bar(toxic_top20, x=toxic_top20.index, y=0,\n",
    "             color_discrete_sequence =['red']*(len(toxic_top20)),\n",
    "             title='Count of Top Words in Toxic Comments', \n",
    "             template='plotly_white', \n",
    "             labels={'index': 'Word', '0': 'Count'})\n",
    "fig.update_traces(marker_color='red')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d34987",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_top20=pd.DataFrame(not_toxic_comments.T.sort_values(ascending=False)[0:50])\n",
    "fig = px.bar(not_toxic_top20, x=not_toxic_top20.index, y=0,\n",
    "             color_discrete_sequence =['red']*(len(toxic_top20)),\n",
    "             title='Count of Top Words in Not Toxic Comments', \n",
    "             template='plotly_white', \n",
    "             labels={'index': 'Word', '0': 'Count'})\n",
    "fig.update_traces(marker_color='blue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e142d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_top20['Label'] = 'Not Toxic'\n",
    "toxic_top20['Label'] = 'Toxic'\n",
    "\n",
    "together_2 = pd.concat([not_toxic_top20, toxic_top20])\n",
    "mapper = {0:'Count'}\n",
    "together_2=together_2.rename(columns=mapper)\n",
    "\n",
    "together_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='model_probability').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82267694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[2481,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='model_probability').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20494ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[2248,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b73e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
