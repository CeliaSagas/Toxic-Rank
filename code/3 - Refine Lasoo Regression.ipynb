{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e735e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv(\"./jigsaw-toxic-severity-rating/validation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ec304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create distinct df's for more toxic and less toxic comments\n",
    "\n",
    "less_toxic = data.drop('more_toxic', axis=1)\n",
    "more_toxic = data.drop('less_toxic', axis =1)\n",
    "\n",
    "#add target column to both \n",
    "\n",
    "less_toxic['target'] = 0\n",
    "more_toxic['target'] = 1\n",
    "\n",
    "#rename text column for both \n",
    "\n",
    "less_toxic = less_toxic.rename(columns={'less_toxic':'text'})\n",
    "more_toxic = more_toxic.rename(columns={'more_toxic':'text'})\n",
    "\n",
    "#rejoin data \n",
    "\n",
    "comments = pd.concat([less_toxic, more_toxic], axis =0)\n",
    "\n",
    "#randomly reorder\n",
    "\n",
    "comments = comments.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#create average target score for comments \n",
    "comments = comments.groupby('text')['target'].mean().reset_index()\n",
    "\n",
    "#shape\n",
    "print(comments.head())\n",
    "\n",
    "#save comments\n",
    "\n",
    "comments.to_csv(\"comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13102823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remap target values \n",
    "\n",
    "comments['target'] = (comments['target'] > 0.0) * 1\n",
    "\n",
    "#histogram of toxic values per comment \n",
    "%matplotlib inline\n",
    "sns.histplot(data=comments, x=\"target\", binwidth=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31949202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text clean + tokenizer function\n",
    "\n",
    "punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~__'\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "cv_tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_token(text):\n",
    "    \n",
    "    \n",
    "    text = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "    text = [word_tokenize(word) for word in text]\n",
    "    text = [item for sublist in text for item in sublist]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = [re.sub(r'http\\S+', '', each) for each in text]\n",
    "    text = [re.sub('[0-9+]', '', each) for each in text]\n",
    "    text = [re.sub('_', '', each) for each in text]\n",
    "    text = [re.sub(\"\\n\",\"\",each) for each in text]\n",
    "    text = [re.sub('/_/g', '', each) for each in text]\n",
    "    text = [re.sub('[^\\u0000-\\u05C0\\u2100-\\u214F]+', '', each) for each in text]\n",
    "    text = [re.sub('[\\u0401\\u0451\\u0410-\\u044f]', '', each) for each in text]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [\"\".join(dict.fromkeys(word)) for word in text]\n",
    "\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068234f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into X and y\n",
    "\n",
    "X, y = comments['text'], comments['target']\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2082243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition data into test data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of all unique comments in the dataset\n",
    "\n",
    "train_corpus = [word for word in X_train]\n",
    "test_corpus = [word for word in X_test]\n",
    "\n",
    "\n",
    "# #tokenize train and test sets \n",
    "\n",
    "train_features = [to_token(comment) for comment in train_corpus]\n",
    "test_features = [to_token(comment) for comment in test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tfidf vector matrix for train and test sets \n",
    "tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_features)\n",
    "test_tfidf = tfidf.transform(test_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use truncated SVD on each tfidf matrix\n",
    "svd = TruncatedSVD(n_components=25, random_state=33)\n",
    "\n",
    "train_svd = svd.fit_transform(train_tfidf)\n",
    "test_svd = svd.transform(test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cceaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale features\n",
    "\n",
    "scale=StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(train_svd)\n",
    "X_test_scaled = scale.transform(test_svd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944084f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for distribution of labels\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create base model\n",
    "lr_basemodel =LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train base model\n",
    "lr_basemodel.fit(X_train_scaled,y_train)\n",
    "y_pred_basemodel = lr_basemodel.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92230328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get base f1 and roc auc score for model\n",
    "print(\"f1 score for base model is : \" , f1_score(y_test,y_pred_basemodel))\n",
    "print(\"ROC AUC score for base model is : \", roc_auc_score(y_test, y_pred_basemodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "# define model/create instance\n",
    "lr=LogisticRegression()\n",
    "#tuning weight for minority class then weight for majority class will be 1-weight of minority class\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,500)\n",
    "#specifying all hyperparameters with possible values\n",
    "param= {'C': [0.1, 0.5, 1,10,15,20], 'penalty': [ 'l2'],\"class_weight\":[{0:x ,1:1.0 -x} for x in weights]}\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "#Gridsearch for hyperparam tuning\n",
    "model= GridSearchCV(estimator= lr,param_grid=param,scoring=\"roc_auc\",cv=folds,return_train_score=True)\n",
    "#train model to learn relationships between x and y\n",
    "model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best ROC AUC score: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Model with best params\n",
    "lr2=LogisticRegression(class_weight={0: 0.6963727454909819, 1: 0.30362725450901806}\n",
    "                       ,C=10,penalty=\"l2\")\n",
    "lr2.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities on Test and take probability for class 1([:1])\n",
    "y_pred_prob_test = lr2.predict_proba(X_test_scaled)[:, 1]\n",
    "#predict labels on test dataset\n",
    "y_pred_test = lr2.predict(X_test_scaled)\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"confusion Matrix is : \\n\\n\", cm)\n",
    "print(f\"\\n\")\n",
    "\n",
    "# ROC- AUC score\n",
    "print(\"ROC-AUC score  test dataset:  \", roc_auc_score(y_test,y_pred_prob_test))\n",
    "#Precision score\n",
    "print(\"precision score  test dataset:  \", precision_score(y_test,y_pred_test))\n",
    "#Recall Score\n",
    "print(\"Recall score  test dataset:  \", recall_score(y_test,y_pred_test))\n",
    "#f1 score\n",
    "print(\"f1 score  test dataset :  \", f1_score(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create toxic probability output df\n",
    "test_output = X_test.copy()\n",
    "test_output['target'] = y_test.tolist()\n",
    "test_output[\"model_probability\"] = y_pred_prob_test.tolist()\n",
    "test_output['prediction'] = y_pred_test.tolist()\n",
    "\n",
    "\n",
    "#create toxic probability output for comments that are mislabeled 1\n",
    "miss_0 =  test_output.loc[(test_output['target'] == 0) & (test_output['model_probability']>0.50)]\n",
    "miss_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create toxic probability output for comments that are mislabeled 0\n",
    "miss_1 =  test_output.loc[(test_output['target'] == 1) & (test_output['model_probability']<0.50)]\n",
    "miss_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7766082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save probability score df\n",
    "\n",
    "test_output.to_csv(\"lr_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    " lr2.coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_1.iloc[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_0.iloc[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3304183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
